{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Bandit with stationary rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# --- Bandit ---\n",
    "class BinaryBandit(object):\n",
    "  def __init__(self):\n",
    "    # N = number of arms\n",
    "    self.N = 2\n",
    "  def actions(self):\n",
    "    result = []\n",
    "    for i in range(0,self.N):\n",
    "      result.append(i)\n",
    "    return result\n",
    "  def reward1(self, action):\n",
    "    p = [0.1, 0.2]\n",
    "    rand = random.random()\n",
    "    if rand < p[action]:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n",
    "  def reward2(self, action):\n",
    "    p = [0.9, 0.8]\n",
    "    rand = random.random()\n",
    "    if rand < p[action]:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBandit = BinaryBandit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eGreedy(myBandit, epsilon, max_iteration):\n",
    "  # Initialization \n",
    "  Q = [0]*myBandit.N \n",
    "  count = [0]*myBandit.N\n",
    "  epsilon = epsilon\n",
    "  r = 0\n",
    "  R = [] #array of rewards\n",
    "  R_avg = [0]*1  #average of rewards recieved till iter i\n",
    "  max_iter = max_iteration\n",
    "  # Incremental Implementation\n",
    "  for iter in range(1,max_iter):\n",
    "    if random.random() > epsilon:\n",
    "      action = Q.index(max(Q)) # Exploit/ Greed\n",
    "    else:\n",
    "      action = random.choice(myBandit.actions()) # Explore\n",
    "    r = myBandit.reward1(action)\n",
    "    R.append(r)\n",
    "    count[action] = count[action]+1\n",
    "    Q[action] = Q[action]+(r - Q[action])/count[action]\n",
    "    R_avg.append(R_avg[iter-1] + (r-R_avg[iter-1])/iter)\n",
    "\n",
    "  return Q, R_avg, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10124164278892064, 0.19906166219839155]\n"
     ]
    }
   ],
   "source": [
    "Q , R_avg , R = eGreedy(myBandit, 0.2,10000)\n",
    "print(Q)\n",
    "#print(R)\n",
    "\n",
    "#print(R_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffa0081c8b0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(R_avg,color= 'red')\n",
    "\n",
    "#it can be seen in the graph that -> for 10000 iterations, \n",
    "\n",
    "#here we are given weights, so the weighted average corresponds to average.\n",
    "#As the average reward is 0.5 for both the actions, this case the choice of action doesn’t matter,\n",
    "#as we expect – on average – to receive the same reward in the long run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10 - arm bandit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a 10-armed bandit in which all ten mean-rewards start out equal and then take independent random walks (by adding a normally distributed increment with mean zero and standard deviation 0.01 to all mean-rewards on each time step). \n",
    "\n",
    "### The 10-armed bandit that you developed (bandit_nonstat) is difficult to crack with a standard epsilon-greedy algorithm since the rewards are non-stationary.  We did discuss how to track non-stationary rewards in class.  Write a modified epsilon-greedy agent and show whether it is able to latch onto correct actions or not.  (Try at least 10000 time steps before commenting on results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for  all the parameters of the algorithm\n",
    "class Params:\n",
    "    def __init__(self):\n",
    "        # number of bandit's arms\n",
    "        self.k = 10\n",
    "\n",
    "        # mean and standard deviation random walk of q_star\n",
    "        self.mu, self.sigma = 0, 0.01\n",
    "\n",
    "        # number of k-armed bandit problems\n",
    "        self.m = None\n",
    "\n",
    "        # step size\n",
    "        self.alpha = 0.1\n",
    "\n",
    "        # step size possible values\n",
    "        self.alphas = [2 ** x for x in range(-5, 2)]\n",
    "\n",
    "        # epsilon-greedy parameter epsilon\n",
    "        self.epsilon = 0.1\n",
    "\n",
    "        # epsilon-greedy parameter epsilon possible values\n",
    "        self.epsilons = [2 ** x for x in range(-7, -1)]\n",
    "\n",
    "        # UCB parameter c possible values\n",
    "        self.cs = [2 ** x for x in range(-4, 3)]\n",
    "\n",
    "        # greedy with optimistic initialization possible initialization values\n",
    "        self.initializations = [2 ** x for x in range(-2, 3)]\n",
    "\n",
    "        # steps of a run\n",
    "        self.steps = None\n",
    "\n",
    "        # number of last steps over the average is computed\n",
    "        self.last_steps_eval = 10000\n",
    "\n",
    "        # names of the algorithms used to solve the k-armed bandit problem\n",
    "        self.bandit_types = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for an algorithm for solving the k-armed bandit problem\n",
    "class Bandit:\n",
    "    def __init__(self, bandit_type, k, epsilon, m, alpha, c=None, initialization=None):\n",
    "        \n",
    "       \n",
    "        # Set up the parameters\n",
    "        self.bandit_type = bandit_type #algorithm used to solve the k-armed bandit problem\n",
    "        self.k = k #number of arms of a bandit\n",
    "        self.epsilon = epsilon\n",
    "        self.m = m  #number of bandits\n",
    "        self.alpha = alpha  #step size\n",
    "        self.c = c\n",
    "        self.initialization = initialization\n",
    "\n",
    "        # Table q_star, real reward mean for each action of each bandit\n",
    "        self.q_star = np.zeros((self.m, self.k))\n",
    "\n",
    "        # Table N, used for sample-average method\n",
    "        self.N = np.zeros((self.m, self.k))\n",
    "\n",
    "        # Time variable, used for UCB method\n",
    "        self.time = 0\n",
    "\n",
    "        # Table Q, estimate reward mean for each action of each bandit (zero if there is no initialization)\n",
    "        if initialization is None:\n",
    "            self.Q = np.zeros((self.m, self.k))\n",
    "        else:\n",
    "            self.Q = np.full((self.m, self.k), initialization)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Choose action for each bandit, observe results and update appropriate tables\n",
    "        :returs the actions taken and reward obtained\n",
    "        \"\"\"\n",
    "        # Sample average action selection algorithm\n",
    "        if self.bandit_type == 'sample_average':\n",
    "            # Choose the action to take for each bandit\n",
    "            A = [np.argmax(row) if np.random.binomial(1, self.epsilon) == 0 else int(np.random.uniform(0, self.k)) for\n",
    "                 row in self.Q[:, ]]\n",
    "\n",
    "            # Observe rewards\n",
    "            R = [np.random.normal(self.q_star[i, value], 1) for i, value in enumerate(A)]\n",
    "\n",
    "            # Update N and Q tables\n",
    "            self.N = np.reshape([self.N[i, :] + Bandit.one_hot(value, self.k) for i, value in enumerate(A)],\n",
    "                                (self.m, self.k))\n",
    "            self.Q = np.reshape([self.Q[i, :] + (1 / self.N[i, value]) * (R[i] - self.Q[i, value]) *\n",
    "                                 Bandit.one_hot(value, self.k) for i, value in enumerate(A)], (self.m, self.k))\n",
    "\n",
    "        # Sample average action selection algorithm with constant step size, with zero or optimistic initialization\n",
    "        elif self.bandit_type == 'constant_step_size' or self.bandit_type == 'optimistic_init':\n",
    "            # Choose the action to take for each bandit\n",
    "            A = [np.argmax(row) if np.random.binomial(1, self.epsilon) == 0 else int(np.random.uniform(0, self.k)) for\n",
    "                 row in self.Q[:, ]]\n",
    "\n",
    "            # Observe rewards\n",
    "            R = [np.random.normal(self.q_star[i, value], 1) for i, value in enumerate(A)]\n",
    "\n",
    "            # Update Q table\n",
    "            self.Q = np.reshape([self.Q[i, :] + self.alpha * (R[i] - self.Q[i, value]) * Bandit.one_hot(value, self.k)\n",
    "                                 for i, value in enumerate(A)], (self.m, self.k))\n",
    "\n",
    "        # Gradient bandit algorithm\n",
    "        elif self.bandit_type == 'gradient':\n",
    "            A_probability = [np.exp(row)/np.sum(np.exp(row)) for row in self.Q[:, ]]\n",
    "\n",
    "            # Choose the action to take for each bandit\n",
    "            A = [np.random.choice(np.arange(self.k), p=p) for p in A_probability]\n",
    "\n",
    "            # Observe rewards\n",
    "            R = [np.random.normal(self.q_star[i, value], 1) for i, value in enumerate(A)]\n",
    "\n",
    "            # Update Q table\n",
    "            self.Q = np.reshape([self.Q[i, :] + self.alpha * R[i] * (Bandit.one_hot(value, self.k) - A_probability[i])\n",
    "                                 for i, value in enumerate(A)], (self.m, self.k))\n",
    "\n",
    "        # Upper-Confidence-Bound action selection algorithm\n",
    "        elif self.bandit_type == 'ucb':\n",
    "            # Choose the action to take for each bandit\n",
    "            A = [np.argmax(row + self.c * np.sqrt(np.log(self.time + 1) / (n + 1))) if\n",
    "                 np.random.binomial(1,self.epsilon) == 0 else int(np.random.uniform(0, self.k)) for row, n in\n",
    "                 zip(self.Q[:, ], self.N[:, ])]\n",
    "\n",
    "            # Observe rewards\n",
    "            R = [np.random.normal(self.q_star[i, value], 1) for i, value in enumerate(A)]\n",
    "\n",
    "            # Update time value\n",
    "            self.time += 1\n",
    "\n",
    "            # Update N and Q tables\n",
    "            self.N = np.reshape([self.N[i, :] + Bandit.one_hot(value, self.k) for i, value in enumerate(A)],\n",
    "                                (self.m, self.k))\n",
    "            self.Q = np.reshape([self.Q[i, :] + (1 / self.N[i, value]) * (R[i] - self.Q[i, value]) *\n",
    "                                 Bandit.one_hot(value, self.k) for i, value in enumerate(A)], (self.m, self.k))\n",
    "\n",
    "        return A, R\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_hot(position, length):\n",
    "        \n",
    "       # position: position of the '1' value\n",
    "        #length: length of the array\n",
    "        \n",
    "        assert length > position, 'position {:d} greater than or equal to length {:d}'.format(position, length)\n",
    "        oh = np.zeros(length)\n",
    "        oh[position] = 1\n",
    "        return oh\n",
    "    \n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(bandit_types, ARs, POAs):\n",
    "    \n",
    "    \n",
    "    # bandit_types: names of the algorithms used to solve the k-armed bandit problem\n",
    "    # ARs: average reward value, for each algorithm and for each step\n",
    "    # POAs: percentage of optimal action chosen value, for each algorithm and for each step\n",
    "    \n",
    "    for bandit_type, AR, POA in zip(bandit_types, ARs, POAs):\n",
    "        plt.figure(0)\n",
    "        plt.plot(AR, label=bandit_type)\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Average reward')\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.plot(POA, label=bandit_type)\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('% Optimal action')\n",
    "        \n",
    "def solution():\n",
    "    # Set up parameters\n",
    "    params = Params()\n",
    "\n",
    "    params.m = 2000\n",
    "    params.steps = 10000\n",
    "    params.bandit_types = ['sample_average', 'constant_step_size']\n",
    "\n",
    "    ARs = []\n",
    "    POAs = []\n",
    "\n",
    "    for bandit_type in params.bandit_types:\n",
    "        print('Method', bandit_type)\n",
    "\n",
    "        # Average Reward\n",
    "        AR = []\n",
    "\n",
    "        # Percentage of Optimal Action\n",
    "        POA = []\n",
    "\n",
    "        # Set up the algorithm\n",
    "        bandit = Bandit(bandit_type, params.k, params.epsilon, params.m, params.alpha)\n",
    "\n",
    "        for _ in range(params.steps):\n",
    "            # Take a step\n",
    "            A, R = bandit.step()\n",
    "\n",
    "            # Collect rewards and compute the percentage of optimal values\n",
    "            AR.append(np.average(R))\n",
    "            POA.append(np.average([100 if value == np.argmax(bandit.q_star[i, :]) else 0 for i, value in enumerate(A)]))\n",
    "\n",
    "            # Change the true value of every arm of every bandit\n",
    "            bandit.q_star += np.random.normal(params.mu, params.sigma, (params.m, params.k))\n",
    "\n",
    "        ARs.append(AR)\n",
    "        POAs.append(POA)\n",
    "\n",
    "    # Show results\n",
    "    show_results(params.bandit_types, ARs, POAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method sample_average\n"
     ]
    }
   ],
   "source": [
    "solution()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
